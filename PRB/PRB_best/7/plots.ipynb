{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prototyping LSS pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joseph/anaconda3/envs/lss/lib/python3.6/site-packages/Bio/KDTree/__init__.py:25: BiopythonDeprecationWarning: Bio.KDTree has been deprecated, and we intend to remove it in a future release of Biopython. Please use Bio.PDB.kdtrees instead, which is functionally very similar.\n",
      "  BiopythonDeprecationWarning)\n",
      "/home/joseph/anaconda3/envs/lss/lib/python3.6/site-packages/MDAnalysis/due.py:88: UserWarning: No module named 'duecredit'\n",
      "  warnings.warn(str(err))\n"
     ]
    }
   ],
   "source": [
    "import os, pickle, time, glob, sys, copy\n",
    "import numpy as np \n",
    "import scipy\n",
    "import mdtraj as md \n",
    "import MDAnalysis as mda\n",
    "import nglview as nv \n",
    "from ipywidgets import interactive, VBox\n",
    "import sklearn.preprocessing as pre\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pyemma as py \n",
    "from pyemma.util.contexts import settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "K.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "#__all__ = [\"hde\", \"propagator\", \"molgen\"]\n",
    "from hde import *#__init__, hde, propagator, molgen, analysis #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## locating trajectory data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/joseph/Memorex USB/liew/Joe_Liew_LSS/Joe_Liew_LSS/PRB/DESRES-Trajectory_PRB-0-protein/PRB-0-protein/system.pdb\n",
      "/media/joseph/Memorex USB/liew/Joe_Liew_LSS/Joe_Liew_LSS/PRB/DESRES-Trajectory_PRB-0-protein/PRB-0-protein/system.pdb\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f5ddbe88c6d4deb8797b70de2ea37e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "NGLWidget()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DESRES Chignolin at 340 K\n",
    "# Lindorff-Larsen et al. Science 334 6055 517-520 (2011)\n",
    "# 200 ps time steps\n",
    "# 53 dcd files each conataining 10,000 frames at 200 ps time steps = 53*10,000*200 = 106 us\n",
    "\n",
    "timestep = 200 # ps\n",
    "\n",
    "pdb_dir = \"/media/joseph/Memorex USB/liew/Joe_Liew_LSS/Joe_Liew_LSS/PRB/DESRES-Trajectory_PRB-0-protein/PRB-0-protein\"\n",
    "pdb_file = os.path.join(pdb_dir, \"system.pdb\")\n",
    "\n",
    "trj_dir = pdb_dir\n",
    "trj_file = []\n",
    "for i in np.arange(0,15):\n",
    "    fname = \"PRB-0-protein-\" + str(i).zfill(3) + \".dcd\"\n",
    "    trj_file.append(os.path.join(trj_dir, fname))\n",
    "print(pdb_file)\n",
    "\n",
    "print(pdb_file)\n",
    "view = nv.show_structure_file(pdb_file)\n",
    "view.clear_representations()\n",
    "view.add_ball_and_stick()\n",
    "view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mdtraj load and view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_obj = md.load(trj_file, top=pdb_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mdtraj.Trajectory with 150000 frames, 737 atoms, 47 residues, and unitcells at 0x7f0e4c0c3c88>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj_obj.center_coordinates(mass_weighted=False)\n",
    "traj_obj.superpose(traj_obj[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c48b4226d34ce0a47b412485ff2008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "NGLWidget(count=150000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view = nv.show_mdtraj(traj_obj)\n",
    "view.clear_representations()\n",
    "view.add_spacefill()\n",
    "view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### h2t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ca_first_idx = traj_obj.topology.select('name CA and resid 0')\n",
    "Ca_last_idx = traj_obj.topology.select('name CA and resid ' + str(traj_obj.n_residues-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2t = md.compute_distances(traj_obj, np.reshape(np.array([Ca_first_idx, Ca_last_idx]), (1,2)), periodic=True, opt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14-05-21 16:11:49 pyemma.coordinates.data.featurization.featurizer.MDFeaturizer[0] WARNING  The 1D arrays input for add_inverse_distances() have been sorted, and index duplicates have been eliminated.\n",
      "Check the output of describe() to see the actual order of the features\n",
      "dim = 10224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joseph/anaconda3/envs/lss/lib/python3.6/site-packages/mdtraj/geometry/dihedral.py:374: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  indices = np.vstack(x for x in indices if x.size)[id_sort]\n",
      "/home/joseph/anaconda3/envs/lss/lib/python3.6/site-packages/pyemma/coordinates/data/featurization/angles.py:211: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  indices = np.vstack(valid.values())\n"
     ]
    }
   ],
   "source": [
    "# featurizing\n",
    "features = py.coordinates.featurizer(pdb_file)\n",
    "features.add_backbone_torsions(cossin=True)\n",
    "features.add_sidechain_torsions(which='all', cossin=True)\n",
    "atom_idx = features.select_Backbone() # select_Heavy() select_Ca()\n",
    "features.add_inverse_distances(atom_idx)\n",
    "\n",
    "#print(features.describe())\n",
    "print('dim = %d' % features.dimension())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) latent space projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag=100\n",
    "dim=4\n",
    "is_reversible=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SRV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/joseph/anaconda3/envs/lss/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/joseph/anaconda3/envs/lss/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/joseph/anaconda3/envs/lss/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/joseph/anaconda3/envs/lss/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/joseph/anaconda3/envs/lss/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#earlyStopping = EarlyStopping(monitor='val_loss', patience=30, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "hde = HDE(\n",
    "    features.dimension(), \n",
    "    n_components=dim, \n",
    "    lag_time=lag,\n",
    "    reversible=is_reversible, \n",
    "    n_epochs=300,\n",
    "    learning_rate=0.0005,\n",
    "    hidden_layer_depth=2,\n",
    "    hidden_size=100,\n",
    "    activation='tanh', \n",
    "    batch_size=20000,\n",
    "    batch_normalization=True,\n",
    "    #callbacks=[earlyStopping], \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/joseph/anaconda3/envs/lss/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/joseph/anaconda3/envs/lss/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/joseph/anaconda3/envs/lss/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/joseph/anaconda3/envs/lss/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joseph/anaconda3/envs/lss/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "hde = pickle.load(open('hde.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "getting output of FeatureReader:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.666666666666667 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "getting output of FeatureReader:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.333333333333334 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfff2d718dc54f1f9b13c6595ce3f614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "getting output of FeatureReader:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = np.zeros((100000,2))\n",
    "for i in range(len(trj_file)):\n",
    "    print(100 * i / len(trj_file), \"%\")\n",
    "    q = py.coordinates.load(trj_file[i], features=features, chunksize = 80)\n",
    "    h = hde.transform(q, side=\"left\")\n",
    "    r = i*10000\n",
    "    l = (i+1*10000)\n",
    "    if(i == 0):\n",
    "        data = h\n",
    "    else:\n",
    "        data = np.concatenate((data, h), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hde_coords = data\n",
    "hde_timescales = hde.timescales_\n",
    "print(hde_timescales)\n",
    "print(hde_coords)\n",
    "print(hde.eigenvalues_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 14}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "\n",
    "heights = -lag/np.log(hde.eigenvalues_)\n",
    "figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.bar(range(len(heights)), heights)\n",
    "plt.xlabel(\"Eigenvalue Index\")\n",
    "plt.ylabel(r'$\\frac{-\\tau}{ln(\\lambda_i)}$')\n",
    "plt.title(r\"$t_i$'s BBA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) propagator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_prop = dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_prop = copy.deepcopy(hde_coords[:,:dim_prop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_scaler = pre.MinMaxScaler(feature_range=(0,1))\n",
    "if dim_prop==1:\n",
    "    traj_prop_scaled = prop_scaler.fit_transform(traj_prop.reshape(-1, 1))\n",
    "else:\n",
    "    traj_prop_scaled = prop_scaler.fit_transform(traj_prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mix = 25\n",
    "lag_prop = lag\n",
    "lr_prop = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='loss', patience=50, restore_best_weights=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop = Propagator(\n",
    "    traj_prop_scaled.shape[1], \n",
    "    n_components=n_mix, \n",
    "    lag_time=lag_prop, \n",
    "    batch_size=200000, \n",
    "    learning_rate=lr_prop, \n",
    "    n_epochs=20000,\n",
    "    callbacks=callbacks,\n",
    "    hidden_size=100,\n",
    "    activation='relu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hde.propagator import get_mixture_loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop.model.compile(loss=get_mixture_loss_func(prop.input_dim, prop.n_components), optimizer=tf.keras.optimizers.Adam(lr=lr_prop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop.model.set_weights(pickle.load(open('prop_weights.pkl', 'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = np.int(np.floor(np.float(hde_coords.shape[0])/np.float(lag_prop)))\n",
    "n_traj = 10\n",
    "synth_trajs_scaled = [prop.propagate(traj_prop_scaled[0].reshape(1,-1).astype(np.float32), n_steps).reshape(n_steps, -1) for item in range(n_traj)]\n",
    "synth_trajs = [prop_scaler.inverse_transform(synth_trajs_scaled[i]) for i in range(n_traj)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_traj_id = 0\n",
    "fig, ax = plt.subplots()\n",
    "id1 = 1\n",
    "id2 = 0\n",
    "if dim_prop==1:\n",
    "    ax.scatter(hde_coords[::lag_prop,0], np.zeros(hde_coords[::lag_prop,id1].shape[0]), c='blue', alpha=0.5, cmap='jet')\n",
    "    ax.scatter(synth_trajs[synth_traj_id][:,0], np.zeros(synth_trajs[synth_traj_id][:,0].shape[0]), c='orange', alpha=0.5, cmap='jet')\n",
    "    ax.set_xlabel(r'$\\psi_1$')\n",
    "    ax.set_ylabel(r'$\\psi_2$')\n",
    "else:\n",
    "    ax.scatter(hde_coords[::lag_prop,id1], hde_coords[::lag_prop,id2], c='blue', alpha=0.5, cmap='jet')\n",
    "    ax.scatter(synth_trajs[synth_traj_id][:,id1], synth_trajs[synth_traj_id][:,id2], c='orange', alpha=0.5, cmap='jet')\n",
    "    ax.set_xlabel(r'$\\psi_1$')\n",
    "    ax.set_ylabel(r'$\\psi_2$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_traj_id = 0\n",
    "psi_id = 1\n",
    "for i in range(n_traj):\n",
    "    synth_traj_id = i\n",
    "    fig, ax = plt.subplots(figsize=(24, 6))\n",
    "    ax.plot(np.arange(hde_coords[::lag_prop,psi_id].shape[0])*timestep/1E6*lag_prop, hde_coords[::lag_prop,psi_id], color='blue', alpha=0.5, marker='o')\n",
    "    ax.plot(np.arange(synth_trajs[synth_traj_id].shape[0])*timestep/1E6*lag_prop, synth_trajs[synth_traj_id][:,psi_id], color='orange', alpha=0.5, marker='o')\n",
    "    ax.set_xlabel(r'$t$ (us)')\n",
    "    ax.set_ylabel(r'$\\psi$'+str(psi_id+1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### x_train = scaled latent space coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = traj_prop_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### y_train = aligned molecular configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_idx = traj_obj.top.select_atom_indices('alpha')\n",
    "traj_ca = traj_obj.atom_slice(ca_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_ca.superpose(traj_ca[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view = nv.show_mdtraj(traj_ca[::lag_prop])\n",
    "#view.component_0.clear_representations()\n",
    "view.component_0.add_ribbon(color='blue', opacity=0.6)\n",
    "view\n",
    "view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz = traj_ca.xyz.reshape(-1, traj_ca.n_atoms*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz_scaler = pre.MinMaxScaler(feature_range=(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = xyz_scaler.fit_transform(xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_atoms = traj_ca.n_atoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training cWGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "molgen = MolGen(\n",
    "    latent_dim=x_train.shape[1],\n",
    "    output_dim=y_train.shape[1],\n",
    "    batch_size=30000,\n",
    "    noise_dim=50,\n",
    "    n_epochs=2500,\n",
    "    hidden_layer_depth=2,\n",
    "    hidden_size=200,\n",
    "    n_discriminator=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x):\n",
    "    return (K.sigmoid(x) * x)\n",
    "molgen.generator = tf.keras.models.load_model('molgen_generator.h5', custom_objects={'swish': swish},compile=False)\n",
    "molgen.discriminator = tf.keras.models.load_model('molgen_discriminator.h5', custom_objects={'swish': swish},compile=False)\n",
    "molgen.is_fitted = True# need to override after loading to allow molgen.transform to proceed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### decoding synthetic propagator trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert molgen.is_fitted == True\n",
    "print(len(synth_trajs_scaled[synth_traj_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_traj_id = 1\n",
    "xyz_synth = xyz_scaler.inverse_transform(molgen.transform(synth_trajs_scaled[synth_traj_id])).reshape(-1, n_atoms, 3)\n",
    "xyz_synth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('synth.xyz', 'w') as f:\n",
    "    for i in range(xyz_synth.shape[0]):\n",
    "        f.write('%d\\n' % n_atoms)\n",
    "        f.write('\\n')\n",
    "        for k in range(n_atoms):\n",
    "            f.write('%3s%17.9f%17.9f%17.9f\\n' % ('C', xyz_synth[i][k][0]*10, xyz_synth[i][k][1]*10, xyz_synth[i][k][2]*10) ) # nm to Angstroms for xyz write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_ca[0].save_pdb('synth.pdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_ca_synth = copy.deepcopy(traj_ca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_ca_synth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_synth_obj = md.load('synth.xyz', top='synth.pdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_synth_obj.center_coordinates(mass_weighted=False)\n",
    "traj_synth_obj.superpose(traj_synth_obj[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view = nv.show_mdtraj(traj_synth_obj)\n",
    "view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### thermodynamics (FES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1D PMF in leading SRV coordinate\n",
    "\n",
    "adding one pseudo-count to each bin as regularization and avoiding infinite free energies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MD 1D PMF mean\n",
    "n_bins = 50\n",
    "pdf_MD, bin_edges = np.histogram(hde_coords[:,0], bins=n_bins, density=True)\n",
    "bin_mids = bin_edges[:-1]+0.5*(bin_edges[1]-bin_edges[0])\n",
    "\n",
    "pdf_MD += 1./(len(hde_coords[:,0])*(bin_edges[1]-bin_edges[0])) # adding one pseudo-count per bin\n",
    "pmf_MD = -np.log(pdf_MD)\n",
    "pmf_MD -= np.mean(pmf_MD[np.isfinite(pmf_MD)]) # optimal mutual least squares alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MD 1D PMF block averages\n",
    "pmf_MD_block = []\n",
    "n_blocks=5\n",
    "for i in range(n_blocks):\n",
    "    \n",
    "    data_i = np.array_split(hde_coords[:,0],n_blocks,axis=0)[i] #block splits\n",
    "    \n",
    "    pdf_MD_i, _ = np.histogram(data_i, bins=bin_edges, density=True)\n",
    "    pdf_MD_i += 1./(len(data_i)*(bin_edges[1]-bin_edges[0])) # adding one pseudo-count per bin\n",
    "    \n",
    "    pmf_MD_i = -np.log(pdf_MD_i)\n",
    "    pmf_MD_i -= np.mean(pmf_MD_i[np.isfinite(pmf_MD_i)]) # optimal mutual least squares alignment\n",
    "    pmf_MD_block.append(pmf_MD_i)\n",
    "    \n",
    "pmf_MD_block = np.array(pmf_MD_block)\n",
    "pmf_MD_stderr = np.std(pmf_MD_block, axis=0)/np.sqrt(n_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shifting global minimum to zero of optimally least squares aligned landscapes\n",
    "PMF_shift = min(pmf_MD)\n",
    "pmf_MD -= PMF_shift\n",
    "for i in range(n_blocks):\n",
    "    pmf_MD_block[i] -= PMF_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting full and block PMFs\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(n_blocks):\n",
    "    ax.plot(bin_mids, pmf_MD_block[i])\n",
    "im = ax.plot(bin_mids, pmf_MD, color='black', linewidth=3)\n",
    "plt.xlabel(r'$\\psi_1$')\n",
    "plt.ylabel(r'$G$ ($k_BT$)')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting full PMFs with 95% CI (1.96 standard deviations in standard error)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(bin_mids, pmf_MD, color='blue')\n",
    "ax.fill_between(bin_mids, pmf_MD-1.96*pmf_MD_stderr, pmf_MD+1.96*pmf_MD_stderr, alpha=0.2, color='blue')\n",
    "plt.xlabel(r'$\\psi_1$')\n",
    "plt.ylabel(r'$G$ ($k_BT$)')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSS 1D PMF mean\n",
    "synth_trajs_CONCAT = np.zeros((0,))\n",
    "for i in range(n_traj):\n",
    "    synth_trajs_CONCAT = np.concatenate((synth_trajs_CONCAT,synth_trajs[i][:,0]), axis=0)\n",
    "pdf_LSS, _ = np.histogram(synth_trajs_CONCAT, bins=n_bins, density=True)\n",
    "\n",
    "pdf_LSS += 1./(len(synth_trajs_CONCAT)*(bin_edges[1]-bin_edges[0])) # adding one pseudo-count per bin\n",
    "pmf_LSS = -np.log(pdf_LSS)\n",
    "pmf_LSS -= np.mean(pmf_LSS[np.isfinite(pmf_LSS)]) # optimal mutual least squares alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSS 1D PMF block averages\n",
    "pmf_LSS_block = []\n",
    "n_blocks=5\n",
    "for i in range(n_blocks):\n",
    "    \n",
    "    data_i = np.array_split(synth_trajs_CONCAT,n_blocks,axis=0)[i] #block splits\n",
    "    \n",
    "    pdf_LSS_i, _ = np.histogram(data_i, bins=bin_edges, density=True)\n",
    "    pdf_LSS_i += 1./(len(data_i)*(bin_edges[1]-bin_edges[0])) # adding one pseudo-count per bin\n",
    "    \n",
    "    pmf_LSS_i = -np.log(pdf_LSS_i)\n",
    "    pmf_LSS_i -= np.mean(pmf_LSS_i[np.isfinite(pmf_LSS_i)]) # optimal mutual least squares alignment\n",
    "    pmf_LSS_block.append(pmf_LSS_i)\n",
    "    \n",
    "pmf_LSS_block = np.array(pmf_LSS_block)\n",
    "pmf_LSS_stderr = np.std(pmf_LSS_block, axis=0)/np.sqrt(n_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shifting global minimum to zero of optimally least squares aligned landscapes\n",
    "PMF_shift = min(pmf_LSS)\n",
    "#pmf_LSS -= PMF_shift\n",
    "for i in range(n_blocks):\n",
    "    pmf_LSS_block[i] -= PMF_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros(len(pmf_LSS_block[0]))\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(n_blocks):\n",
    "    x += pmf_LSS_block[i]/n_blocks\n",
    "    ax.plot(bin_mids, pmf_LSS_block[i])\n",
    "im = ax.plot(bin_mids, x, color='black', linewidth=3)\n",
    "plt.xlabel(r'$\\psi_1$')\n",
    "plt.ylabel(r'$G$ ($k_BT$)')\n",
    "plt.show\n",
    "print(pmf_LSS_block[1])\n",
    "pmf_LSS = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(bin_mids, pmf_LSS, color='orange')\n",
    "ax.fill_between(bin_mids, pmf_LSS-pmf_LSS_stderr, pmf_LSS+pmf_LSS_stderr, alpha=0.2, color='orange')\n",
    "plt.xlabel(r'$\\psi_1$')\n",
    "plt.ylabel(r'$G$ ($k_BT$)')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(bin_mids, pmf_MD, color='blue', label='MD')\n",
    "ax.fill_between(bin_mids, pmf_MD-1.96*pmf_MD_stderr, pmf_MD+1.96*pmf_MD_stderr, alpha=0.2, color='blue')\n",
    "ax.plot(bin_mids, pmf_LSS, color='orange', label='LSS')\n",
    "ax.fill_between(bin_mids, pmf_LSS-1.96*pmf_LSS_stderr, pmf_LSS+1.96*pmf_LSS_stderr, alpha=0.2, color='orange')\n",
    "plt.xlabel(r'$\\psi_1$')\n",
    "plt.ylabel(r'$G$ ($k_BT$)')\n",
    "ax.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> $\\psi_2$ thermo </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MD 1D PMF mean\n",
    "psi_index = 1\n",
    "n_bins = 50\n",
    "pdf_MD, bin_edges = np.histogram(hde_coords[:,psi_index], bins=n_bins, density=True)\n",
    "bin_mids = bin_edges[:-1]+0.5*(bin_edges[1]-bin_edges[0])\n",
    "\n",
    "pdf_MD += 1./(len(hde_coords[:,psi_index])*(bin_edges[1]-bin_edges[0])) # adding one pseudo-count per bin\n",
    "pmf_MD = -np.log(pdf_MD)\n",
    "pmf_MD -= np.mean(pmf_MD[np.isfinite(pmf_MD)]) # optimal mutual least squares alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MD 1D PMF block averages\n",
    "pmf_MD_block = []\n",
    "n_blocks=5\n",
    "for i in range(n_blocks):\n",
    "    \n",
    "    data_i = np.array_split(hde_coords[:,psi_index],n_blocks,axis=0)[i] #block splits\n",
    "    \n",
    "    pdf_MD_i, _ = np.histogram(data_i, bins=bin_edges, density=True)\n",
    "    pdf_MD_i += 1./(len(data_i)*(bin_edges[1]-bin_edges[0])) # adding one pseudo-count per bin\n",
    "    \n",
    "    pmf_MD_i = -np.log(pdf_MD_i)\n",
    "    pmf_MD_i -= np.mean(pmf_MD_i[np.isfinite(pmf_MD_i)]) # optimal mutual least squares alignment\n",
    "    pmf_MD_block.append(pmf_MD_i)\n",
    "    \n",
    "pmf_MD_block = np.array(pmf_MD_block)\n",
    "pmf_MD_stderr = np.std(pmf_MD_block, axis=0)/np.sqrt(n_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shifting global minimum to zero of optimally least squares aligned landscapes\n",
    "PMF_shift = min(pmf_MD)\n",
    "pmf_MD -= PMF_shift\n",
    "for i in range(n_blocks):\n",
    "    pmf_MD_block[i] -= PMF_shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kinetic (relaxation times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_autocorr_time(signal, delays):\n",
    "    ac=[]\n",
    "    for j in delays:\n",
    "        if type(signal) is list:\n",
    "            z = np.concatenate([item[:] for item in signal])\n",
    "            x = np.concatenate([item[:-j] for item in signal])\n",
    "            y = np.concatenate([item[j:] for item in signal])\n",
    "        else:\n",
    "            z = signal[:]\n",
    "            x = signal[:-j]\n",
    "            y = signal[j:]\n",
    "        ac.append(np.mean(x*y, axis=0)/np.mean(z*z, axis=0))\n",
    "    return np.array(ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MD mean autocorrelation and implied relaxation time\n",
    "ac_MD = compute_autocorr_time(hde_coords[:,:dim_prop], [lag_prop])\n",
    "relax_MD = -lag_prop/np.log(ac_MD)*timestep/1E3\n",
    "print(ac_MD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MD block average standard error estimate\n",
    "ac_MD_block = []\n",
    "n_blocks=5\n",
    "for i in range(n_blocks):\n",
    "    ac_MD_i = compute_autocorr_time(np.array_split(hde_coords[:,:dim_prop],n_blocks,axis=0)[i], [lag_prop])\n",
    "    ac_MD_block.append(ac_MD_i)\n",
    "ac_MD_block = np.array(ac_MD_block)\n",
    "ac_MD_block = ac_MD_block.reshape(n_blocks,dim_prop)\n",
    "\n",
    "relax_MD_block = -lag_prop/np.log(ac_MD_block)*timestep/1E3\n",
    "relax_MD_stderr = np.std(relax_MD_block, axis=0)/np.sqrt(n_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MD relaxation times (ns) = ')\n",
    "for i in range(relax_MD.shape[1]):\n",
    "    print('%f +/- %f' % (relax_MD[0,i], relax_MD_stderr[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSS mean autocorrelation and implied relaxation time\n",
    "ac_LSS = compute_autocorr_time(synth_trajs, [1])\n",
    "relax_LSS = -lag_prop/np.log(ac_LSS)*timestep/1E3\n",
    "print(ac_LSS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSS block average standard error estimate\n",
    "# N.B. Blocks in this case are over complete indepenent LSS trajectories since data must be continuous\n",
    "\n",
    "assert n_traj >= 5 # need at least 5 independent trajectories for reasonable blocks\n",
    "\n",
    "ac_LSS_block = []\n",
    "for i in range(n_traj):\n",
    "    ac_LSS_i = compute_autocorr_time(synth_trajs[i], [1])\n",
    "    ac_LSS_block.append(ac_LSS_i)\n",
    "ac_LSS_block = np.array(ac_LSS_block)\n",
    "ac_LSS_block = ac_LSS_block.reshape(n_traj,synth_trajs[0].shape[1])\n",
    "\n",
    "relax_LSS_block = -lag_prop/np.log(ac_LSS_block)*timestep/1E3\n",
    "relax_LSS_stderr = np.std(relax_LSS_block, axis=0)/np.sqrt(n_traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LSS relaxation times (ns) = ')\n",
    "for i in range(relax_LSS.shape[1]):\n",
    "    print('%f +/- %f' % (relax_LSS[0,i], relax_LSS_stderr[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### structure (RMSD)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TO DO: \n",
    "Follow protocol in Chem. Sci., 2020, 11, 9459 (77.pdf) - \"Structures\" on p. 9464\n",
    "\n",
    "- extract MD and LSS structures in the metastable basins and compare their relative Calpha RMSD using mdtraj or MDAnalysis libraries; note that this is a pairwise calculation -- given N_MD structures and N_LSS structures within a particular metastable basin, what is the mean value of RMSD_ij where i=1..N_MD and j=1..N_LSS\n",
    "\n",
    "- for this protein system we have two metastable states at \\psi_1 ~ (-1.3) and (+0.7) so we should analyze the structures in both these basins by extracting snapshots from the MD and LSS trajectories with \\psi_1 values of, say, [-1.4, -1.2] and [0.6, 0.8]. Depending on how many frames you extract, you may wish to loosen or tighten these bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xyz_synth = xyz_scaler.inverse_transform(molgen.transform(synth_trajs_scaled[synth_traj_id])).reshape(-1, n_atoms, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeToXYZ(xyz_coordinates, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for i in range(xyz_coordinates.shape[0]):\n",
    "            f.write('%d\\n' % n_atoms)\n",
    "            f.write('\\n')\n",
    "            for k in range(n_atoms):\n",
    "                f.write('%3s%17.9f%17.9f%17.9f\\n' % ('C', xyz_synth[i][k][0]*10, xyz_synth[i][k][1]*10, xyz_synth[i][k][2]*10) ) # nm to Angstroms for xyz write\n",
    "\n",
    "                \n",
    "def xyzToTraj(xyz_coordinates,fn=\"temp.xyz\"):\n",
    "    writeToXYZ(xyz_coordinates, fn)\n",
    "    traj = md.load(fn, top='synth.pdb')\n",
    "    return traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    ~~~RMSD Calculation~~~\n",
    "    k = frames where the indexed LSS trajectory is in the metastable basin1\n",
    "    l = frames where the MD obj trajectory is in the mestable basin1\n",
    "    \n",
    "    n = frames where the indexed LSS trajectory is in the metastable basin2\n",
    "    m = frames where the MD obj trajectory is in the mestable basin2\n",
    "\n",
    "    t = traj containing all corresponding LSS frames for basin 1\n",
    "    s = traj containing all corresponding LSS frames for basin 2\n",
    "    \n",
    "'''\n",
    "lowerBasin1 = -1.9\n",
    "upperBasin1 = -1.5\n",
    "\n",
    "lowerBasin2 = 0.5\n",
    "upperBasin2 = 0.9\n",
    "\n",
    "\n",
    "l = np.where(np.logical_and(hde_coords[:,0]>=lowerBasin1, hde_coords[:,0]<=upperBasin1))\n",
    "m = np.where(np.logical_and(hde_coords[:,0]>=lowerBasin2, hde_coords[:,0]<=upperBasin2))\n",
    "\n",
    "\n",
    "for i in range(len(synth_trajs)):\n",
    "    print(\"Finding Frames for Traj \",i)\n",
    "    k = np.where(np.logical_and(synth_trajs[i][:,0]>=lowerBasin1, synth_trajs[0][:,0]<=upperBasin1))\n",
    "    n = np.where(np.logical_and(synth_trajs[i][:,0]>=lowerBasin2, synth_trajs[0][:,0]<=upperBasin2))\n",
    "    xyzi = xyz_scaler.inverse_transform(molgen.transform(synth_trajs_scaled[i])).reshape(-1, n_atoms, 3)\n",
    "    if(i == 0): t = xyzToTraj(xyzi)[k]\n",
    "    else: t = md.join([t,xyzToTraj(xyzi)[k]])\n",
    "    if(i == 0): s = xyzToTraj(xyzi)[n]\n",
    "    else: s = md.join([t,xyzToTraj(xyzi)[n]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsd1 = np.mean(md.rmsd(t, traj_ca[l]))\n",
    "rmsd2 = np.mean(md.rmsd(s, traj_ca[m]))\n",
    "print(\"Basin One RMSD is \", rmsd1, \" nm. Found \", len(t), \"LSS frames in Basin One and \", len(l[0]), \" MD frames\")\n",
    "print(\"Basin Two RMSD is \", rmsd2, \" nm. Found \", len(s), \"LSS frames in Basin Two and \", len(m[0]), \" MD frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.logical_and(hde_coords[:,0]>=lowerBasin2, hde_coords[:,0]<=upperBasin2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "67734/23695"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
